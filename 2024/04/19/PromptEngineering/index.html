<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Kai">





<title>了解提示工程 | 面向兴趣编程</title>



    <link rel="icon" href="/icoimage.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 6.3.0"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">anonymity&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">anonymity&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6;    // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function () {
            tocbot.refresh(obj_merge(tocbot_default_config, { hasInnerContainers: true }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function () {
        tocbot.init(obj_merge(tocbot_default_config, { collapseDepth: 1 }));
    });

    function expandToc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, { collapseDepth: expanded ? 1 : DEPTH_MAX }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">了解提示工程</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Kai</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">April 19, 2024&nbsp;&nbsp;16:17:36</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/AI/">AI</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h3 id="什么是提示工程？"><a href="#什么是提示工程？" class="headerlink" title="什么是提示工程？"></a>什么是提示工程？</h3><p>提示工程(Prompt Engineering)用于提升大语言模型(Large Language Model, LLM)在处理复杂任务场景的能力，比如问答或推理计算能力。这是一门比较新的学科，随着AI应用的爆发性增长而进入大众视野。</p>
<p>提示工程在与大模型交互，对接，以及理解大模型预言能力等方面发挥着重要作用，我们熟知的ChatGPT就使用了提示工程来提高多轮对话的效果。</p>
<p>比较现实的是，工程师个人难以拥有深度学习所需的算力，无法提供模型训练和微调所需的资源，在某些特定场景下提示工程可能是门槛较低的优化大模型方案之一。</p>
<p>提示词由四个要素组成：</p>
<p><strong>指令：</strong>想要模型执行的任务</p>
<p><strong>上下文：</strong>给模型提供一些额外的上下文信息，用于引导模型输出更优质的内容</p>
<p><strong>输入数据：</strong>用户输入的内容或问题</p>
<p><strong>输出指示：</strong>指定模型输出的类型或格式</p>
<blockquote>
<p>展示一段代码</p>
<p>要求输出”hello word”</p>
<p>编程语言为Go语言</p>
</blockquote>
<p>大模型输出:</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">以下是一个简单的 Go 代码段，用于输出 “hello world”：</span><br><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> <span class="string">&quot;fmt&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    fmt.Println(<span class="string">&quot;hello world&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上例示例中，指令或任务是”展示一段代码”，输出数据为”要求输出’hello word’”,输出指示为”编程语言为Go语言”。上例示例中并没有包含<strong>上下文</strong>，但是大模型依然输出了我们期望的内容，可见提示词并非上述元素都是必须的。同时通过大模型的输出，我们可以发现”要求输出’hello word’”,被大模型输出为<code>fmt.Println(&quot;hello world&quot;)</code>,模型矫正了单词错误。</p>
<p>在使用提示词时，一般工程师会通过API调用的方式或直接与大模型进行交互，在这个阶段可以通过配置一些参数用来优化输出结果。不同参数对于输出的结果非常重要，常见的一些参数设置：</p>
<p><strong>Temperature</strong>:这个参数值越小，模型返回的结果越精确，这个参数越大，模型返回的随机性越大。在一些需要精确结果的应用场景下可以将这个值调小，反之在一些创造性强的场景下，例如生成诗歌，编一个故事…可以适当通过调高参数值以丰富模型输出。</p>
<p><strong>Max Length</strong>: 这个参数用于控制大模型Token输出，可以避免大模型生成冗长或无意义的响应。</p>
<p><strong>Stop Sequences：</strong>用来控制大模型输出的另一种方式，是一个string类型，设置可以控制列表输出项等。</p>
<p><strong>Frequency Penalty:</strong> 用来调整多轮对话中，某个词在模型输出中出现的次数，该参数越高，某个词再次出现的可能性就越小。</p>
<p>上例参数是大模型可调参数的一部分，具体可根据大模型<a target="_blank" rel="noopener" href="https://open.bigmodel.cn/dev/api#glm-4">相关文档</a>进行阅读，本文所有示例基于GLM-4，但不确保模型响应与本文示例保持一致。</p>
<p>在OpenAI官网提供的提示工程指南中提出了六个原则：</p>
<ul>
<li>写出清晰的指令</li>
<li>提供参考文本</li>
<li>将复杂的任务拆分成简单的子任务</li>
<li>给模型’思考’的时间</li>
<li>使用外部工具</li>
<li>系统地测试变更</li>
</ul>
<h3 id="零样本提示和少样本提示"><a href="#零样本提示和少样本提示" class="headerlink" title="零样本提示和少样本提示"></a>零样本提示和少样本提示</h3><p>经过大量训练出来的大模型已经能够执行零样本任务，即不提供任何示例，模型依然会输出相对符合期望的结果，这源于RLHF技术在深度学习时让模型更好适应人类偏好。但在更复杂的场景或问题下，零样本下的大模型无法输出更优质的结果，这便衍生出少量样本提示来引导模型。</p>
<p>零样本提示：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：&quot;请用&#x27;槑雷&#x27;造句&quot;</span><br><span class="line"></span><br><span class="line">输出：我今天在网吧玩游戏，结果连续遇到槑雷的队友，真是让人哭笑不得。</span><br></pre></td></tr></table></figure>

<p>少量样本提示：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">输入: &#x27;槑雷&#x27;是一种生活用品，一个使用&#x27;槑雷&#x27;这个词的句子的例子是：在洗碗的时候使用槑雷，能让碗变得非常干净&quot;</span><br><span class="line"></span><br><span class="line">	   &#x27;雷槑&#x27;指一段非常好的代码，请用&#x27;雷槑&#x27;造句</span><br><span class="line"></span><br><span class="line">输出:  编写程序时，我用了一段雷槑的代码，让整个程序运行效率大幅提升，性能检测结果让人非常满意。</span><br></pre></td></tr></table></figure>

<p>我们通过上述例子可以观察到，’槑雷’这个人为臆造的词在零样本生成下可能词不达意，但是在少样本提示下，大模型通过示例已经学会了如何正确使用该词。</p>
<p>标签和格式也会对大模型的输出起作用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">输入： 你代码写的太好啦！// 夸赞</span><br><span class="line">      你代码写的太差了！// 批评</span><br><span class="line">      你的裙子真不错！//</span><br><span class="line">      </span><br><span class="line">输出：夸赞</span><br></pre></td></tr></table></figure>

<p>即使是随机标签也会对输出有帮助，毕竟有总比没有好。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">输入：这组数字中的奇数加起来是一个偶数：151、321、51、131、821、71、121。</span><br><span class="line">输出：让我们将给定的奇数相加，并检查其和是否为偶数。</span><br><span class="line">     151 + 321 + 51 + 131 + 821 + 71 + 121 = 1766</span><br><span class="line">     现在，我们来确定这个结果是否为偶数。一个数如果可以被2整除，那么它就是偶数。在这种情况下，1766可以被2整除，因为其个位数是6（偶数的个位数总是0, 2, 4, 6, 或8）。</span><br><span class="line">     因此，这些奇数相加的和1766确实是一个偶数。这符合数学规律，即两个奇数相加总是得到一个偶数（因为两个奇数相加，其个位数总是两个奇数个位数相加等于偶数的结果，从而使得和为偶数），所以多个奇数相加的和也有可能是偶数。在这个例子中，七个奇数相加得到了一个偶数。</span><br></pre></td></tr></table></figure>

<p>可以拿计算器试一下，大模型这方面是反直觉的，用户可能认为基础的计算应该是百无一失的，实际上<code>151 + 321 + 51 + 131 + 821 + 71 + 121</code>并不等于 <code>1766</code>而是 <code>1677</code>，七个奇数相加怎么会得出偶数结果呢？这个结果与大模型后续输出的思考内容也是相悖的。</p>
<p>即使提供少样本提示也很难获得这种类型的推理问题的可靠响应。</p>
<h3 id="思维链提示"><a href="#思维链提示" class="headerlink" title="思维链提示"></a>思维链提示</h3><p>当任务过于复杂，少样本提示无法起作用时。可以使用思维链(CoT)提示，有一个更简单的方式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：151+321+51+131+121+71+821 结果是多少？是奇数还是偶数？让我们逐步思考</span><br><span class="line">输出：我们可以把所有的数相加：151 + 321 + 51 + 131 + 121 + 71 + 821，最终结果是1667。</span><br><span class="line">     1667不是2的倍数，因此它是一个奇数。</span><br></pre></td></tr></table></figure>

<p>“让我们逐步思考”提示是一种自动化思维链展示，在没有太多的示例提供时它相当好用。假设你使用人工手动解析这一计算过程，也可能会输入错误信息(毕竟人类也会犯错，当然，现在大模型也会)而误导大模型。这种自动思维链的方式可以消除人工的方法，当然这种自动的过程也会出错，所以演示的多样性依旧重要：</p>
<p>Auto-CoT 两个阶段：</p>
<p>1.问题聚类：给问题划分为几个聚类</p>
<p>2.演示抽样，从每组数据中选出一个具有代表性的问题，并使用带有简单启发式的 Zero-Shot-CoT 生成其推理链。</p>
<p>简单的启发式方法可以是问题的长度（例如，60 个 tokens）和理由的步骤数（例如，5 个推理步骤）。这鼓励模型使用简单而准确的演示。</p>
<h3 id="链式思考-CoT-提示"><a href="#链式思考-CoT-提示" class="headerlink" title="链式思考(CoT)提示"></a>链式思考(CoT)提示</h3><p>还记得最开始OpenAI推荐的六个原则吗？将复杂的任务拆解成简单的子任务。就像人类在面对上述计算题会将逐步数字逐步拆分，在将多个结果整合后给出最终答案。所谓的链式提示就是将子任务的提示词给语言模型。</p>
<blockquote>
<p>LLM 的一个常见用途是根据大型文本文档回答问题。想要更好阅读大文本文档，可以设计两个不同的提示，第一个提示负责提取相关引文以回答问题，第二个提示则以引文和原始文档为输入来回答给定的问题。换句话说，可以创建两个不同的提示来执行根据文档回答问题的任务。</p>
</blockquote>
<p>提示词：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你是一个文档助手，可以根据文档回答问题，你的任务是从文档中提取核心论点，由####分隔。请用&lt;&gt;&lt;/&gt;输出内容。如果无法提取，请回答&quot;我不知道&quot;！</span><br><span class="line">###</span><br><span class="line">文档内容详情为：https://kubernetes.io/zh-cn/docs/concepts/overview/#why-you-need-kubernetes-and-what-can-it-do</span><br><span class="line">###</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">为什么需要 Kubernetes，它能做什么？</span><br><span class="line">Kubernetes 提供了弹性运行分布式系统的框架。</span><br><span class="line">功能包括：服务发现和负载均衡、存储编排、自动部署和回滚、自动装箱计算、自我修复、密钥与配置管理、批处理执行、水平扩缩、IPv4/IPv6 双栈、为可扩展性设计。</span><br><span class="line">Kubernetes 不是传统的 PaaS 系统，它保留了用户的选择权，有更高的灵活性。</span><br><span class="line">Kubernetes 不限制支持的应用程序类型，不部署源代码，不提供应用程序级别服务，不是日志记录或警报的解决方案，不采用任何全面的机器配置或管理系统。Kubernetes 不仅仅是一个编排系统，它消除了编排的需要，通过独立可组合的控制过程，将当前状态驱动到预期状态。</span><br></pre></td></tr></table></figure>

<p>然后将第一次的输出以提示词的方式作为第二个提示的输入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">你是一个文档助手，可以根据文档回答问题。你的任务是告知Kubernetes能做什么，由####分隔。请用&lt;&gt;&lt;/&gt;输出内容。如果无法提取，请回答&quot;我不知道&quot;！</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">Kubernetes 提供了弹性运行分布式系统的框架。</span><br><span class="line">功能包括：服务发现和负载均衡、存储编排、自动部署和回滚、自动装箱计算、自我修复、密钥与配置管理、批处理执行、水平扩缩、IPv4/IPv6 双栈、为可扩展性设计。</span><br><span class="line">Kubernetes 不是传统的 PaaS 系统，它保留了用户的选择权，有更高的灵活性。</span><br><span class="line">Kubernetes 不限制支持的应用程序类型，不部署源代码，不提供应用程序级别服务，不是日志记录或警报的解决方案，不采用任何全面的机器配置或管理系统。Kubernetes 不仅仅是一个编排系统，它消除了编排的需要，通过独立可组合的控制过程，将当前状态驱动到预期状态。</span><br><span class="line">###</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Kubernetes 提供了弹性运行分布式系统的框架。功能包括：服务发现和负载均衡、存储编排、自动部署和回滚、自动装箱计算、自我修复、密钥与配置管理、批处理执行、水平扩缩、IPv4/IPv6 双栈、为可扩展性设计。Kubernetes 不仅仅是一个编排系统，它消除了编排的需要，通过独立可组合的控制过程，将当前状态驱动到预期状态。它不限制支持的应用程序类型，不部署源代码，不是传统的 PaaS 系统，保留了用户的选择权和灵活性。</span><br></pre></td></tr></table></figure>

<p>链式提示在构建LLM驱动的对话助手时特别有用，它能在多轮对话中不断优化输出性能，在这个过程中也会涉及到提示词技术中的”生成知识提示”，可以人为地将一部分知识作为提示词输入给模型，从而优化质量。</p>
<p>基于链式思考提示，也衍生出思维树(ToT)框架引导模型将思维当作中间步骤来解决一些通用问题，这有点回归到数学，通过广度优先搜索或深度优先搜索来对每一步思维进行抽样，然后以多轮对话搜索树的方式来增强大模型的能力。</p>
<p>ToT提示词的例子如下(来源于<a target="_blank" rel="noopener" href="https://github.com/dave1010/tree-of-thought-prompting">Hulbert </a>)：</p>
<blockquote>
<p>假设三位不同的专家来回答这个问题。<br>所有专家都写下他们思考这个问题的第一个步骤，然后与大家分享。<br>然后，所有专家都写下他们思考的下一个步骤并分享。<br>以此类推，直到所有专家写完他们思考的所有步骤。<br>只要大家发现有专家的步骤出错了，就让这位专家离开。<br>请问…</p>
</blockquote>
<h3 id="检索增强生成-RAG"><a href="#检索增强生成-RAG" class="headerlink" title="检索增强生成(RAG)"></a>检索增强生成(RAG)</h3><p>通用语言模型仅仅通过微调就可以完成一部分常见任务，本文中使用智谱GLM-4大模型在<strong>自我一致性</strong>和<strong>零样本提示</strong>下已经表现的超出预期，但是依旧无法完成复杂的知识密集形的任务。</p>
<p>检索增强生成(Retrieval Augmented Generation,RAG)用来优化这类知识密集形的任务。RAG把一个信息检索组件和文本生成模型结合在一起，可以简单理解为”给大模型联网”，以<strong>生成知识提示</strong>的方式来优化大模型的输出。同时RAG可以进行调整，而不需要对整个模型进行重新训练，这不仅节约了算力&#x2F;时间等资源，也节约了成本。</p>
<p>RAG会先接受输出，按照输入检索出一组相关的资料文档，然后将这些文档作为上下文和输入的原始提示词结合，然后再输入给大模型得到最终输出。这样大模型可以不用重新训练也可以获取新的知识，这非常有效，同时也对传统的搜索引擎巨头提出挑战。有一些AI搜索的应用可以让用户提炼出更加准确的条目并且避免了广告。例如秘塔，必应等。</p>
<p>RAG已经是一种可行的方案，在知识密集型任务中越来越流行，LangChain文档中<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/use_cases/question_answering/quickstart/">Quickstart</a>记录了如何帮助构建一个RAG应用。</p>
<h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>提示工程虽然是一门较新的学科，但它的发展速度(论文发布速度)却让人咂舌，即使不做AI应用相关的开发，作者本人也推荐大家看一下提示工程相关的资料，这也有助于提高输入质量，能从AI产品中获取更准确，能有更质量的输出。</p>
<h3 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h3><p><a target="_blank" rel="noopener" href="https://www.promptingguide.ai/zh">https://www.promptingguide.ai/zh</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/amazon-science/auto-cot">https://github.com/amazon-science/auto-cot</a></p>
<p><a target="_blank" rel="noopener" href="https://docs.anthropic.com/claude/docs/chain-prompts">https://docs.anthropic.com/claude/docs/chain-prompts</a></p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Kai</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="https://ananonymousfriend.github.io/2024/04/19/PromptEngineering/">https://ananonymousfriend.github.io/2024/04/19/PromptEngineering/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2022 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span></span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/AI/"># AI</a>
                    
                        <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"># 学习笔记</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2024/04/24/Kubernetes%20%E5%AE%89%E5%85%A8/">Kubernetes 安全实践</a>
            
            
            <a class="next" rel="next" href="/2024/04/11/ChatGPT%E6%B5%85%E6%9E%90/">ChatGPT浅析？</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Kai | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>